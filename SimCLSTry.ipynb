{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "095818d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-24 15:37:27.351447: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "import model\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from compare_mt.rouge.rouge_scorer import RougeScorer\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "from utils import Recorder\n",
    "from data_utils import to_cuda, collate_mp, ReRankingDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from functools import partial\n",
    "from model import RankingLoss\n",
    "import math\n",
    "import logging\n",
    "logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"transformers.tokenization_utils_base\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"transformers.tokenization_utils_fast\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d44e291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_setting(args):\n",
    "    args.batch_size = getattr(args, 'batch_size', 1)\n",
    "    args.epoch = getattr(args, 'epoch', 5)\n",
    "    args.report_freq = getattr(args, \"report_freq\", 100)\n",
    "    args.accumulate_step = getattr(args, \"accumulate_step\", 12)\n",
    "    args.margin = getattr(args, \"margin\", 0.01)\n",
    "    args.gold_margin = getattr(args, \"gold_margin\", 0)\n",
    "    args.model_type = getattr(args, \"model_type\", 'roberta-base')\n",
    "    args.warmup_steps = getattr(args, \"warmup_steps\", 10000)\n",
    "    args.grad_norm = getattr(args, \"grad_norm\", 0)\n",
    "    args.seed = getattr(args, \"seed\", 970903)\n",
    "    args.no_gold = getattr(args, \"no_gold\", False)\n",
    "    args.pretrained = getattr(args, \"pretrained\", None)\n",
    "    args.max_lr = getattr(args, \"max_lr\", 2e-3)\n",
    "    args.scale = getattr(args, \"scale\", 1)\n",
    "    args.datatype = getattr(args, \"datatype\", \"diverse\")\n",
    "    args.dataset = getattr(args, \"dataset\", \"xsum\")\n",
    "    args.max_len = getattr(args, \"max_len\", 120)  # 120 for cnndm and 80 for xsum\n",
    "    args.max_num = getattr(args, \"max_num\", 16)\n",
    "    args.cand_weight = getattr(args, \"cand_weight\", 1)\n",
    "    args.gold_weight = getattr(args, \"gold_weight\", 1)\n",
    "\n",
    "\n",
    "def evaluation(args):\n",
    "    # load data\n",
    "    base_setting(args)\n",
    "    tok = RobertaTokenizer.from_pretrained(args.model_type)\n",
    "    collate_fn = partial(collate_mp, pad_token_id=tok.pad_token_id, is_test=True)\n",
    "    test_set = ReRankingDataset(f\"./{args.dataset}/{args.datatype}/test\", args.model_type, is_test=True, maxlen=512, is_sorted=False, maxnum=args.max_num, is_untok=True)\n",
    "    # collate_fun here use bert padding preprocessed data\n",
    "    dataloader = DataLoader(test_set, batch_size=8, shuffle=False, num_workers=4, collate_fn=collate_fn)\n",
    "    # build models\n",
    "    model_path = args.pretrained if args.pretrained is not None else args.model_type\n",
    "    scorer = model.ReRanker(model_path, tok.pad_token_id)\n",
    "    if args.cuda:\n",
    "        scorer = scorer.cuda()\n",
    "    scorer.load_state_dict(torch.load(os.path.join(\"./cache\", args.model_pt), map_location=f'cuda:{args.gpuid[0]}'))\n",
    "    scorer.eval()\n",
    "    model_name = args.model_pt.split(\"/\")[0]\n",
    "\n",
    "    def mkdir(path):\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "\n",
    "    print(model_name)\n",
    "    mkdir(\"./result/%s\"%model_name)\n",
    "    mkdir(\"./result/%s/reference\"%model_name)\n",
    "    mkdir(\"./result/%s/candidate\"%model_name)\n",
    "    rouge_scorer = RougeScorer(['rouge1', 'rouge2', 'rougeLsum'], use_stemmer=True)\n",
    "    rouge1, rouge2, rougeLsum = 0, 0, 0\n",
    "    cnt = 0\n",
    "    acc = 0\n",
    "    scores = []\n",
    "    with torch.no_grad():  # 不做训练，no_grad\n",
    "        for (i, batch) in enumerate(dataloader):\n",
    "            if args.cuda:\n",
    "                to_cuda(batch, args.gpuid[0])\n",
    "            samples = batch[\"data\"]\n",
    "            output = scorer(batch[\"src_input_ids\"], batch[\"candidate_ids\"], batch[\"tgt_input_ids\"])\n",
    "            similarity, gold_similarity = output['score'], output['summary_score']\n",
    "            similarity = similarity.cpu().numpy()\n",
    "            if i % 100 == 0:\n",
    "                print(f\"test similarity: {similarity[0]}\")\n",
    "            max_ids = similarity.argmax(1)\n",
    "            scores.extend(similarity.tolist())\n",
    "            acc += (max_ids == batch[\"scores\"].cpu().numpy().argmax(1)).sum()\n",
    "            for j in range(similarity.shape[0]):\n",
    "                sample = samples[j]\n",
    "                sents = sample[\"candidates\"][max_ids[j]][0]\n",
    "                score = rouge_scorer.score(\"\\n\".join(sample[\"abstract\"]), \"\\n\".join(sents))\n",
    "                rouge1 += score[\"rouge1\"].fmeasure\n",
    "                rouge2 += score[\"rouge2\"].fmeasure\n",
    "                rougeLsum += score[\"rougeLsum\"].fmeasure\n",
    "                with open(\"./result/%s/candidate/%d.dec\"%(model_name, cnt), \"w\") as f:\n",
    "                    for s in sents:\n",
    "                        print(s, file=f)\n",
    "                with open(\"./result/%s/reference/%d.ref\"%(model_name, cnt), \"w\") as f:\n",
    "                    for s in sample[\"abstract\"]:\n",
    "                        print(s, file=f)\n",
    "                cnt += 1\n",
    "    rouge1 = rouge1 / cnt\n",
    "    rouge2 = rouge2 / cnt\n",
    "    rougeLsum = rougeLsum / cnt\n",
    "    print(f\"accuracy: {acc / cnt}\")\n",
    "    print(\"rouge1: %.6f, rouge2: %.6f, rougeL: %.6f\"%(rouge1, rouge2, rougeLsum))\n",
    "\n",
    "\n",
    "def test(dataloader, scorer, args, gpuid):\n",
    "    scorer.eval()\n",
    "    loss = 0\n",
    "    cnt = 0\n",
    "    rouge_scorer = RougeScorer(['rouge1', 'rouge2', 'rougeLsum'], use_stemmer=True)\n",
    "    rouge1, rouge2, rougeLsum = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for (i, batch) in enumerate(dataloader):\n",
    "            if args.cuda:\n",
    "                to_cuda(batch, gpuid)\n",
    "            samples = batch[\"data\"]\n",
    "            output = scorer(batch[\"src_input_ids\"], batch[\"candidate_ids\"], batch[\"tgt_input_ids\"])\n",
    "            similarity, gold_similarity = output['score'], output['summary_score']\n",
    "            similarity = similarity.cpu().numpy()\n",
    "            if i % 1000 == 0:\n",
    "                print(f\"test similarity: {similarity[0]}\")\n",
    "            max_ids = similarity.argmax(1)\n",
    "            for j in range(similarity.shape[0]):\n",
    "                cnt += 1\n",
    "                sample = samples[j]\n",
    "                sents = sample[\"candidates\"][max_ids[j]][0]\n",
    "                score = rouge_scorer.score(\"\\n\".join(sample[\"abstract\"]), \"\\n\".join(sents))\n",
    "                rouge1 += score[\"rouge1\"].fmeasure\n",
    "                rouge2 += score[\"rouge2\"].fmeasure\n",
    "                rougeLsum += score[\"rougeLsum\"].fmeasure\n",
    "    rouge1 = rouge1 / cnt\n",
    "    rouge2 = rouge2 / cnt\n",
    "    rougeLsum = rougeLsum / cnt\n",
    "    scorer.train()\n",
    "    loss = 1 - ((rouge1 + rouge2 + rougeLsum) / 3)\n",
    "    print(f\"rouge-1: {rouge1}, rouge-2: {rouge2}, rouge-L: {rougeLsum}\")\n",
    "    \n",
    "    if len(args.gpuid) > 1:\n",
    "        loss = torch.FloatTensor([loss]).to(gpuid)\n",
    "        dist.all_reduce(loss, op=dist.reduce_op.SUM)\n",
    "        loss = loss.item() / len(args.gpuid)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def run(rank, args):\n",
    "    base_setting(args)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    random.seed(args.seed)\n",
    "    gpuid = args.gpuid[rank]\n",
    "    is_master = rank == 0\n",
    "    is_mp = len(args.gpuid) > 1\n",
    "    world_size = len(args.gpuid)\n",
    "    if is_master:\n",
    "        id = len(os.listdir(\"./cache\"))\n",
    "        recorder = Recorder(id, args.log)\n",
    "    tok = RobertaTokenizer.from_pretrained(args.model_type)\n",
    "    collate_fn = partial(collate_mp, pad_token_id=tok.pad_token_id, is_test=False)\n",
    "    collate_fn_val = partial(collate_mp, pad_token_id=tok.pad_token_id, is_test=True)\n",
    "    train_set = ReRankingDataset(f\"./{args.dataset}/{args.datatype}/train\", args.model_type, maxlen=args.max_len, maxnum=args.max_num)\n",
    "    val_set = ReRankingDataset(f\"./{args.dataset}/{args.datatype}/val\", args.model_type, is_test=True, maxlen=512, is_sorted=False, maxnum=args.max_num)\n",
    "    if is_mp:\n",
    "        train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "    \t train_set, num_replicas=world_size, rank=rank, shuffle=True)\n",
    "        dataloader = DataLoader(train_set, batch_size=args.batch_size, shuffle=False, num_workers=4, collate_fn=collate_fn, sampler=train_sampler)\n",
    "        val_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "    \t val_set, num_replicas=world_size, rank=rank)\n",
    "        val_dataloader = DataLoader(val_set, batch_size=8, shuffle=False, num_workers=4, collate_fn=collate_fn_val, sampler=val_sampler)\n",
    "    else:\n",
    "        dataloader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True, num_workers=4, collate_fn=collate_fn)\n",
    "        val_dataloader = DataLoader(val_set, batch_size=8, shuffle=False, num_workers=4, collate_fn=collate_fn_val)\n",
    "    # build models\n",
    "    model_path = args.pretrained if args.pretrained is not None else args.model_type\n",
    "    scorer = model.ReRanker(model_path, tok.pad_token_id)\n",
    "    if len(args.model_pt) > 0:\n",
    "        # 如果与预训练就加载权重\n",
    "        scorer.load_state_dict(torch.load(os.path.join(\"./cache\", args.model_pt), map_location=f'cuda:{gpuid}'))\n",
    "    if args.cuda:\n",
    "        if len(args.gpuid) == 1:\n",
    "            scorer = scorer.cuda()\n",
    "        else:\n",
    "            dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "            scorer = nn.parallel.DistributedDataParallel(scorer.to(gpuid), [gpuid], find_unused_parameters=True)\n",
    "    scorer.train() # model.train 开始训练，batchnorm 和 dropout启用  \n",
    "                    # model.eval() & model.train()不同mode下 batchnorm和dropout会有影响\n",
    "    # model 开始训练，定义optimizer和lr\n",
    "    init_lr = args.max_lr / args.warmup_steps\n",
    "    s_optimizer = optim.Adam(scorer.parameters(), lr=init_lr)\n",
    "    if is_master:\n",
    "        recorder.write_config(args, [scorer], __file__)\n",
    "    minimum_loss = 100\n",
    "    all_step_cnt = 0\n",
    "    # start training\n",
    "    for epoch in range(args.epoch):\n",
    "        s_optimizer.zero_grad()\n",
    "        step_cnt = 0\n",
    "        sim_step = 0\n",
    "        avg_loss = 0\n",
    "        for (i, batch) in enumerate(dataloader):\n",
    "            if args.cuda:\n",
    "                to_cuda(batch, gpuid)\n",
    "            step_cnt += 1\n",
    "            output = scorer(batch[\"src_input_ids\"], batch[\"candidate_ids\"], batch[\"tgt_input_ids\"])\n",
    "            similarity, gold_similarity = output['score'], output['summary_score']\n",
    "            loss = args.scale * RankingLoss(similarity, gold_similarity, args.margin, args.gold_margin, args.gold_weight)\n",
    "            loss = loss / args.accumulate_step\n",
    "            avg_loss += loss.item()\n",
    "            loss.backward()\n",
    "            if step_cnt == args.accumulate_step:\n",
    "                # optimize step      \n",
    "                if args.grad_norm > 0:\n",
    "                    nn.utils.clip_grad_norm_(scorer.parameters(), args.grad_norm)\n",
    "                step_cnt = 0\n",
    "                sim_step += 1\n",
    "                all_step_cnt += 1\n",
    "                lr = args.max_lr * min(all_step_cnt ** (-0.5), all_step_cnt * (args.warmup_steps ** (-1.5)))\n",
    "                for param_group in s_optimizer.param_groups:\n",
    "                    param_group['lr'] = lr\n",
    "                s_optimizer.step()\n",
    "                s_optimizer.zero_grad()\n",
    "            if sim_step % args.report_freq == 0 and step_cnt == 0 and is_master:\n",
    "                print(\"id: %d\"%id)\n",
    "                print(f\"similarity: {similarity[:, :10]}\")\n",
    "                if not args.no_gold:\n",
    "                    print(f\"gold similarity: {gold_similarity}\")\n",
    "                recorder.print(\"epoch: %d, batch: %d, avg loss: %.6f\"%(epoch+1, sim_step, \n",
    "                 avg_loss / args.report_freq))\n",
    "                recorder.print(f\"learning rate: {lr:.6f}\")\n",
    "                recorder.plot(\"loss\", {\"loss\": avg_loss / args.report_freq}, all_step_cnt)\n",
    "                recorder.print()\n",
    "                avg_loss = 0\n",
    "            del similarity, gold_similarity, loss\n",
    "\n",
    "            if all_step_cnt % 1000 == 0 and all_step_cnt != 0 and step_cnt == 0:\n",
    "                loss = test(val_dataloader, scorer, args, gpuid)\n",
    "                if loss < minimum_loss and is_master:\n",
    "                    minimum_loss = loss\n",
    "                    if is_mp:\n",
    "                        recorder.save(scorer.module, \"scorer.bin\")\n",
    "                    else:\n",
    "                        recorder.save(scorer, \"scorer.bin\")\n",
    "                    recorder.save(s_optimizer, \"optimizer.bin\")\n",
    "                    recorder.print(\"best - epoch: %d, batch: %d\"%(epoch, i / args.accumulate_step))\n",
    "                if is_master:\n",
    "                    recorder.print(\"val rouge: %.6f\"%(1 - loss))\n",
    "               \n",
    "\n",
    "def main(args):\n",
    "    # set env\n",
    "    if len(args.gpuid) > 1:\n",
    "        os.environ['MASTER_ADDR'] = 'localhost'\n",
    "        os.environ['MASTER_PORT'] = f'{args.port}'\n",
    "        mp.spawn(run, args=(args,), nprocs=len(args.gpuid), join=True)\n",
    "    else:\n",
    "        run(0, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0089762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed\n",
    "torch.manual_seed(970903)\n",
    "torch.cuda.manual_seed_all(970903)\n",
    "np.random.seed(970903)\n",
    "random.seed(970903)\n",
    "gpuid = 0\n",
    "is_master =  True # rank == 0\n",
    "is_mp = False #len(args.gpuid) > 1\n",
    "world_size = 0#len(args.gpuid)\n",
    "if is_master:\n",
    "    id = len(os.listdir(\"./cache\"))\n",
    "    recorder = Recorder(id, \"store_true\") #args.log\n",
    "tok = RobertaTokenizer.from_pretrained('roberta-base') # args.model_type\n",
    "\n",
    "# partial 构造新的函数，可以控制部分参数一样，如下就构建了两个函数，有不同的is_test 参数\n",
    "collate_fn = partial(collate_mp, pad_token_id=tok.pad_token_id, is_test=False)\n",
    "collate_fn_val = partial(collate_mp, pad_token_id=tok.pad_token_id, is_test=True)\n",
    "\n",
    "train_set = ReRankingDataset(\"testtargetdatadir/\",'roberta-base', maxlen=120, maxnum=16)\n",
    "val_set = ReRankingDataset(\"testtargetdatadir/\", 'roberta-base', is_test=True, \n",
    "                           maxlen=512, is_sorted=False, maxnum=16)\n",
    "#    if is_mp:\n",
    "#        train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "#    \t train_set, num_replicas=world_size, rank=rank, shuffle=True)\n",
    "#        dataloader = DataLoader(train_set, batch_size=args.batch_size, shuffle=False, num_workers=4, collate_fn=collate_fn, sampler=train_sampler)\n",
    "#        val_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "#    \t val_set, num_replicas=world_size, rank=rank)\n",
    "#        val_dataloader = DataLoader(val_set, batch_size=8, shuffle=False, num_workers=4, collate_fn=collate_fn_val, sampler=val_sampler)\n",
    "\n",
    "dataloader = DataLoader(train_set, batch_size=1, shuffle=True, num_workers=4, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_set, batch_size=8, shuffle=False, num_workers=4, collate_fn=collate_fn_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1da9b2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2022-11-24 15:42:18.808413: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-24 15:42:23.617059: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-24 15:42:28.281043: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-24 15:42:33.026035: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (596 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (893 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 5\n",
      "similarity: tensor([[0.9912, 0.9915, 0.9907]], grad_fn=<SliceBackward0>)\n",
      "gold similarity: tensor([0.9882], grad_fn=<SumBackward1>)\n",
      "epoch: 1, batch: 0, avg loss: 0.000003\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgold similarity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgold_similarity\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m recorder\u001b[38;5;241m.\u001b[39mprint(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, batch: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, avg loss: \u001b[39m\u001b[38;5;132;01m%.6f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m%\u001b[39m(epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, sim_step, \n\u001b[1;32m     53\u001b[0m  avg_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[0;32m---> 54\u001b[0m recorder\u001b[38;5;241m.\u001b[39mprint(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m recorder\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: avg_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1000\u001b[39m}, all_step_cnt)\n\u001b[1;32m     56\u001b[0m recorder\u001b[38;5;241m.\u001b[39mprint()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lr' is not defined"
     ]
    }
   ],
   "source": [
    "model_path = 'roberta-base'\n",
    "scorer = model.ReRanker(model_path, tok.pad_token_id)\n",
    "#if len(args.model_pt) > 0:\n",
    "#    scorer.load_state_dict(torch.load(os.path.join(\"./cache\", args.model_pt), map_location=f'cuda:{gpuid}'))\n",
    "#if args.cuda:\n",
    "#    if len(args.gpuid) == 1:\n",
    "#        scorer = scorer.cuda()\n",
    "#    else:\n",
    "#        dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "#        scorer = nn.parallel.DistributedDataParallel(scorer.to(gpuid), [gpuid], find_unused_parameters=True)\n",
    "scorer.train()\n",
    "init_lr = 2e-3 / 10000\n",
    "s_optimizer = optim.Adam(scorer.parameters(), lr=init_lr)\n",
    "#if is_master:\n",
    "    #recorder.write_config(args, [scorer], __file__)\n",
    "minimum_loss = 100\n",
    "all_step_cnt = 0\n",
    "# start training\n",
    "for epoch in range(5):\n",
    "    s_optimizer.zero_grad()\n",
    "    step_cnt = 0\n",
    "    sim_step = 0\n",
    "    avg_loss = 0\n",
    "    for (i, batch) in enumerate(dataloader):\n",
    "        #if args.cuda:\n",
    "        #    to_cuda(batch, gpuid)\n",
    "        step_cnt += 1\n",
    "        output = scorer(batch[\"src_input_ids\"], batch[\"candidate_ids\"], batch[\"tgt_input_ids\"])\n",
    "        similarity, gold_similarity = output['score'], output['summary_score']\n",
    "        loss = 1 * RankingLoss(similarity, gold_similarity, 0.01, 0, 1)\n",
    "        loss = loss / 12\n",
    "        avg_loss += loss.item()\n",
    "        loss.backward()\n",
    "        # mimic large batch size\n",
    "        if step_cnt == 12:\n",
    "            # optimize step      \n",
    "            if 0 > 0:\n",
    "                nn.utils.clip_grad_norm_(scorer.parameters(), 0)\n",
    "            step_cnt = 0\n",
    "            sim_step += 1\n",
    "            all_step_cnt += 1\n",
    "            lr = 2e-3 * min(all_step_cnt ** (-0.5), all_step_cnt * (10000 ** (-1.5)))\n",
    "            for param_group in s_optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            s_optimizer.step()\n",
    "            s_optimizer.zero_grad()\n",
    "        if sim_step % 1 == 0 :\n",
    "            print(\"id: %d\"%id)\n",
    "            print(f\"similarity: {similarity[:, :10]}\")\n",
    "            if not False:\n",
    "                print(f\"gold similarity: {gold_similarity}\")\n",
    "            recorder.print(\"epoch: %d, batch: %d, avg loss: %.6f\"%(epoch+1, sim_step, \n",
    "             avg_loss / 1000))\n",
    "            recorder.print(f\"learning rate: {lr:.6f}\")\n",
    "            recorder.plot(\"loss\", {\"loss\": avg_loss / 1000}, all_step_cnt)\n",
    "            recorder.print()\n",
    "            avg_loss = 0\n",
    "        del similarity, gold_similarity, loss\n",
    "\n",
    "        if all_step_cnt % 1 == 0 and all_step_cnt != 0 and step_cnt == 0:\n",
    "            #loss = test(val_dataloader, scorer, args, gpuid)\n",
    "            if loss < minimum_loss and is_master:\n",
    "                minimum_loss = loss\n",
    "                if is_mp:\n",
    "                    recorder.save(scorer.module, \"scorer.bin\")\n",
    "                else:\n",
    "                    recorder.save(scorer, \"scorer.bin\")\n",
    "                recorder.save(s_optimizer, \"optimizer.bin\")\n",
    "                recorder.print(\"best - epoch: %d, batch: %d\"%(epoch, i / 10000))\n",
    "            if is_master:\n",
    "                recorder.print(\"val rouge: %.6f\"%(1 - loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5626bf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main函数是用来训练打分模型的，所以需要的输入是"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "varInspector": {
   "cols": {
    "lenName": "20",
    "lenType": "20",
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
